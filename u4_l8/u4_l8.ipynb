{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This is incomplete. I am having trouble calculating the F1 using `precision_recall_fscore_support` for kfold.\n",
    "I think this is because when using kfold, the crosstab does not yield categories with 0 counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 4 Lesson 8: Evaluating classifier performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimated time: 5 - 6 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we're going to dive a little deeper into evaluating estimator performance. We're familiar with some common measure of estimator performance like accuracy, precision, recall, and F1 scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit 4 Lesson 8 Assignment 1: Evaluating Classifier Performance Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the unit we've been splitting our data into training, test, and validation sets. Let's take a moment and discuss why this is necessary. By now you can probably see that learning an estimator and testing that estimator's performance on the same data is a methodological mistake. It's like if a professor administered a test with the exact same questions as the practice test. All a student would have to do to get 100% would be to memorize all the solutions to the practice test; they wouldn't actually have to learn anything. If you test your estimator on the data used to train it, it knows all the answers, and thus can achieve a perfect score, even though it very well could fail to predict anything on new data. This is called overfitting. Predicting on never-before-seen data is kind of the whole point, so knowing how our estimator performs on data it has already seen isn't really useful.\n",
    "\n",
    "Holding out a subset of your data for testing, i.e., excluding a subset of your data from your training set, gives you some never-before-seen data to test your estimator's performance. The scikit-learn library has a train_test_split helper function to randomly split data into training and test sets.\n",
    "\n",
    "When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and we can't make claims about how it will generalize (i.e., how it will perform) on never-before-seen data.\n",
    "\n",
    "To resolve this problem, we can hold out yet another subset of our data for validation. Training proceeds on the training set, evaluation is done on the validation set, and when it seems like we have a good model, we can perform our final evaluation on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Classifier      | Advantages              | Shortcomings           | Uses                                  |\n",
    "| --------------- | ----------------------- | -----------------------|---------------------------------------|\n",
    "|Regression \t  | Fast, Easy to interpret |\tAccuracy is average  |\tPredicting continuous numeric values |\n",
    "| Decision Trees  |\tFast, Easy to understand|\tAccuracy is average, Cannot handle large sets of predictors |\tUsed when \"explaining\" the decisions is important |\n",
    "| Random Forests |\tBetter accuracy, Scalable |\tSlow model building, complex |\tClassification problems with large number of predictors |\n",
    "| Naive Bayes |\tSimple, Works with less data | Accuracy, Missing data handling |\tText classification |\n",
    "| K-Nearest Neighbors |\tSimple, No model building step, less computational costs |\tHigher storage requirements during prediction, Cannot handle large datasets |\tStock market forecasting, Credit rating,Customer profiling |\n",
    "| Support Vector Machines |\tKernel method, works well in fuzzy situations |\tNot easy to explain predictions |\tFace recognition, handwriting recognition |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the `cross_validation.train_test_split()` helper function to split the Iris dataset into training and test sets, holding out 40% of the data for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = pd.read_csv('/home/MZ/Documents/CODE/THINKFUL/projects/kaggle/ARCHIVE/iris_wiki.csv')\n",
    "feature_names = ['Sepal length', 'Sepal width', 'Petal length', 'Petal width']\n",
    "features = iris.columns[:4] # Index(['Sepal length', 'Sepal width', 'Petal length', 'Petal width'], dtype='object')\n",
    "\n",
    "y, _ = pd.factorize(iris['Species']) # Encode input values as an enumerated type or categorical variable\n",
    "iris[\"y\"] = y\n",
    "train, test = train_test_split(iris, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many points do you have in your training set? In your test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 60\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear Support Vector Classifier to the training set and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(train[features], train[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Species</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0       setosa  versicolor  virginica\n",
       "Species                                  \n",
       "setosa          23           0          0\n",
       "versicolor       0          19          0\n",
       "virginica        0           0         18"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names = np.array(['setosa', 'versicolor', 'virginica'])\n",
    "preds = target_names[clf.predict(test[features])] # array(['setosa', 'versicolor', 'setosa', 'setosa', 'setos...\n",
    "pd.crosstab(test['Species'], preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0, None)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target_names[y]\n",
    "# preds\n",
    "precision_recall_fscore_support(target_names[test[\"y\"]], preds, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...How does it compare to the score in the Support Vector Machine lesson?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 4 Lesson 8 Project 2: Cross Validation\n",
    "### Estimated Time: 1 - 2 hours\n",
    "The more data we set aside for testing and validation, the less data we have for training, and this will negatively impact estimator performance. To resolve this problem, we can use cross validation (see lesson 4.1.5) to \"recycle\" data over different folds. In this assignment, we're going to implement 5-fold cross-validation on the Iris dataset to train and test a Support Vector Machine classifier.\n",
    "Try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the 5-fold cross-validation score of the SVC from the last assignment [in this unit]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1(ct, preds):\n",
    "    '''Returns the F1 score given the confusion matrix generated with a set of predictions'''\n",
    "#     index = list(pd.crosstab(iris['y'], preds))\n",
    "    total = 0\n",
    "    for i in list(ct):\n",
    "        total += sum(list(ct[i]))\n",
    "\n",
    "    true_positives, false_positives, true_negatives, false_negatives = 0, 0, 0, 0\n",
    "\n",
    "    for i in zip(list(ct), range(1, len(ct)+1)): \n",
    "        row = list(ct.ix[i[0]])\n",
    "        column = list(ct[i[0]])\n",
    "    \n",
    "        tp = row[i[1]-1]\n",
    "        true_positives  += tp\n",
    "        false_negatives += sum(row)-tp\n",
    "        false_positives += sum(column) - tp\n",
    "        true_negatives = total - tp\n",
    "    \n",
    "    return float(2*true_positives) / (2 * true_positives + false_positives + false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "\n",
      "1.0\n",
      "\n",
      "0.928571428571\n",
      "\n",
      "0.966666666667\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'versicolor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-262-15ab6791fc0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Species'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-260-628deb6fbce0>\u001b[0m in \u001b[0;36mf1\u001b[1;34m(ct, preds)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mcolumn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mct\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/MZ/anaconda2/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/MZ/anaconda2/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m    965\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/MZ/anaconda2/lib/python3.5/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no slices here, handle elsewhere'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/MZ/anaconda2/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, copy, drop_level)\u001b[0m\n\u001b[0;32m   1484\u001b[0m                                                       drop_level=drop_level)\n\u001b[0;32m   1485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1486\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/MZ/anaconda2/lib/python3.5/site-packages/pandas/core/index.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   1757\u001b[0m                                  'backfill or nearest lookups')\n\u001b[0;32m   1758\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1759\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1761\u001b[0m         indexer = self.get_indexer([key], method=method,\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3979)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3843)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12265)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12216)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'versicolor'"
     ]
    }
   ],
   "source": [
    "# Code adapted from u4_l1_p5\n",
    "\n",
    "k = 5\n",
    "n = len(iris)\n",
    "kf = KFold(n, n_folds=k) \n",
    "\n",
    "clf = svm.SVC()\n",
    "target_names_numeric = np.array([0, 1, 2])\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    train, test = iris.ix[train_index], iris.ix[test_index]\n",
    "    clf.fit(train[features], train[\"y\"])\n",
    "    preds = target_names[clf.predict(test[features])] # array(['setosa', 'versicolor', 'setosa', 'setosa', 'setos...\n",
    "    \n",
    "    ct = pd.crosstab(test['Species'], preds)\n",
    "    print(f1(ct, preds))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the mean score and the standard deviation of the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the sklean documentation notes, the default score computed at each cross-validation iteration is the estimator's accuracy. We could tell it to return the F1 score, precision, or recall instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do the accuracy scores compare to the F1 scores for this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more detail on the different scoring parameters, see The scoring parameter: defining model evaluation rules.\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit your code as \"cv.py\" via the link below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 4 Lesson 8 Assignment 3: Discussion\n",
    "### Estimated Time 1 - 2 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"How do I know if my model fits the data well?\"\n",
    "\n",
    "This is an important question that you should be asking yourself. A lot. And while it may feel like a silly thing to ask, evaluating the fit of a model is not as straightforward as it may seem. Many evaluation metrics have been developed to assess the fit of models, but it's important to keep in mind that each of these metrics was designed with a subset of models in mind, and probably evaluates different aspects of a model than another metric as a result.\n",
    "\n",
    "A really good example of this are the metrics used to evaluate the fit of binary classification models. One way of evaluating the fit of a binary model is with a contingency table. Contingency tables break up the predicted classes against the actual classes like so:\n",
    "\n",
    "| type |\tpredict 0 |\tpredicted 1 |\n",
    "| ---- | ------------ | ----------- |\n",
    "| actual 0 |\t2 |\t1 |\n",
    "| actual 1 |\t2 |\t1 |\n",
    "\n",
    "In this example contingency table, 3 observations were correctly predicted and 3 observations were incorrectly predicted. This gives us a percentage correctly predicted (PCP) score (another metric for evaluating binary classifiers) of 50%. As you might recall from the Classification, Regression, and Prediction lesson, that these metrics depend on the probability threshold we set to coerce continuous predictions to categorical predictions. If we change that threshold, our contingency table and out PCP score could change too. Additionally, we can lose information about the continuous predictions of the model itself. There's a difference between classifying an observation as 1 because the predicted probability was 0.51 or because the predicted probability was 0.99, but there's no distinction between these two cases in a contingency table or a PCP score.\n",
    "\n",
    "A Receiver Operating Characteristic (ROC) curve is another way of evaluating the fit of a binary model. The ROC curve shows, graphically, the trade off between false-positives and false-negatives as the probability threshold for classifying observations is varied. The overall predictive power of a model is captured in the area under the ROC curve, termed the AUC (\"Area Under Curve\") score, and falls somewhere between 0 and 1. A model that performs no better than a coin toss would have an AUC score of 0.5. The limitation of the ROC curve in evaluating a model is that the shape of the curve doesn't tell us a lot about our model. All we have to go on is the area underneath it. How would we compare a ROC curve that's high on the left side with a ROC curve that's high on the right side if they both have the same AUC?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do a little research on Brier Scores, Expected Percentage of Correct Predictions, and Pseudo R2 measures. What features of binary model fit do they capture? What do they miss?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to evaluate a model isn't written in stone anywhere, but computing multiple metrics tells you more than one single metric will. Computing these metrics is part of the practice of performing model diagnostics, and is useful not only for you as you're testing hypotheses and tuning models, but also for others to see your iterative modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference / scaffolding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: code from # Code adapted from u4_l1_p5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # TRAIN\n",
    "    it_train = pd.Series(intrate).iloc[min(train):max(train)] # interest rate\n",
    "    la_train = loanamt.iloc[min(train):max(train)] # loan amount\n",
    "    fi_train = fico.iloc[min(train):max(train)] # fico\n",
    "\n",
    "    x1_train = np.matrix(fi_train).transpose()     # fico is a series; transpose converts vertically\n",
    "    x2_train = np.matrix(la_train).transpose()\n",
    "\n",
    "    train_X = np.column_stack([x1_train,x2_train])\n",
    "    train_y = np.matrix(it_train).transpose()\n",
    "\n",
    "    X_train = sm.add_constant(train_X)\n",
    "\n",
    "    model = sm.OLS(train_y,X_train)\n",
    "    f = model.fit()   \n",
    "    # print f.summary()    \n",
    "\n",
    "    # TEST \n",
    "    it_test = pd.Series(intrate).iloc[min(test):max(test)]\n",
    "    la_test = loanamt.iloc[min(test):max(test)]\n",
    "    fi_test = fico.iloc[min(test):max(test)]\n",
    "\n",
    "    x1_test = np.matrix(fi_test).transpose()\n",
    "    x2_test = np.matrix(la_test).transpose()\n",
    "\n",
    "    test_X = np.column_stack([x1_test,x2_test])\n",
    "\n",
    "    X_test = sm.add_constant(test_X)    \n",
    "\n",
    "    y_actual = test_y = np.matrix(intrate).transpose()[test[0]:test[-1]] # is this right?\n",
    "\n",
    "    y_predicted = f.predict(X_test)\n",
    "    \n",
    "    mse += mean_squared_error(y_actual, y_predicted) \n",
    "    mas += mean_absolute_error(y_actual, y_predicted)\n",
    "    r2  += r2_score(y_actual, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.22222222222222221, 0.33333333333333331, 0.26666666666666666, None)\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n",
    "y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n",
    "print(precision_recall_fscore_support(y_true, y_pred, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
