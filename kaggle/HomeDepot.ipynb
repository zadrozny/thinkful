{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from math import log\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product_descriptions = pd.read_csv('DATA/product_descriptions.csv', header=0, encoding = \"ISO-8859-1\")\n",
    "attributes = pd.read_csv('DATA/attributes.csv', header=0, encoding = \"ISO-8859-1\")\n",
    "train = pd.read_csv('DATA/train.csv', header=0,  encoding = \"ISO-8859-1\")\n",
    "test = pd.read_csv('DATA/test.csv', header=0,  encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATOR SCORES (NORMALIZED)\n",
    "* Evaluator scores are are continuous averages on the interval [1, 3]; the classifier seems unable to handles this\n",
    "* relevance_rounded consists of values: 1, 2, 3\n",
    "* relevance_rounded_quarters consists of values 1, 1.25, 1.5...3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since relevance is an aggregate, values are continuous rather than discrete\n",
    "# It appears the classifier can only handle a limited number of values\n",
    "# So we round up\n",
    "train['relevance_rounded'] = train['relevance'].map(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def round_custom(x):\n",
    "    '''Rounds relevance to quarters -- for greater granularity'''\n",
    "    whole, fraction = int(x), 100 * float(x - int(x))\n",
    "\n",
    "    if fraction < 12.5:   fraction =  0\n",
    "    elif fraction < 37.5: fraction = 25\n",
    "    elif fraction < 62.5: fraction = 50\n",
    "    elif fraction < 87.5: fraction = 75\n",
    "    else: fraction = 100\n",
    "\n",
    "    return whole + fraction / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['relevance_rounded_quarters'] = train['relevance'].map(lambda x: round_custom(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: EXACT MATCH\n",
    "* between search term (as phrase) and product title, allowing only differences in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Produces a boolean\n",
    "\n",
    "train['exact'] = pd.Series([True if st.lower() in pt.lower() else False \n",
    "                      for st, pt in zip(train['search_term'], train['product_title'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: OVERLAPPING WORDS\n",
    "* allowing for differences in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Produces a count >= 0\n",
    "\n",
    "train['overlapping_words'] = pd.Series([len(set(st.lower().split()) & set(pt.lower().split()))\n",
    "                      for st, pt in zip(train['search_term'], train['product_title'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: PERCENTAGE OVERLAPPING WORDS\n",
    "* allowing for differences in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['percentage_overlapping_words'] = pd.Series([len(set(st.lower().split()) & \n",
    "                                                       set(pt.lower().split()))/float(len(st.split()))\n",
    "                                                       for st, pt in zip(train['search_term'], train['product_title'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc(doc, query, N=1, D={}):\n",
    "    '''Calculates TF-IDF score for doc, query pair, given a document count (N) and dictionary of word counts (D)'''\n",
    "    doc = [w.lower() for w in doc.split()]\n",
    "    counts = Counter(doc)\n",
    "    query = query.lower().split()\n",
    "    return sum([counts[word] * log(N / (1+D[word])) for word in set(query) & set(doc)]) # 1 to avoid div by 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF: product descriptions (all) + product titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of documents\n",
    "N = len(set(product_descriptions['product_uid'] + train['product_uid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dictionary of word-count pairs\n",
    "words_product_descriptions = ''.join(product_descriptions['product_description']).split()\n",
    "words_train = ''.join(train['product_title']).split()\n",
    "D = Counter(words_product_descriptions + words_train) # Not lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "# frequent = sorted(d.items(), key=lambda x: x[1])[:50:-1]\n",
    "# frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = OrderedDict()\n",
    "\n",
    "for row in zip(train['id'], train['product_uid'], train['product_title'], train['search_term']):\n",
    "    _id, pid, pt, st = row\n",
    "    TF = Counter(pt.split())\n",
    "    score = sum([calc(pt, word, N, D) for word in st.lower().split()])\n",
    "    scores[_id] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                              2\n",
       "product_uid                                                100001\n",
       "product_title                   Simpson Strong-Tie 12-Gauge Angle\n",
       "search_term                                         angle bracket\n",
       "relevance                                                       3\n",
       "relevance_rounded                                               3\n",
       "relevance_rounded_quarters                                      3\n",
       "exact                                                       False\n",
       "overlapping_words                                               1\n",
       "percentage_overlapping_words                                  0.5\n",
       "tf_idf                                                    4.16932\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector\n",
    "tf_idf = list(scores.values())\n",
    "train['tf_idf'] = tf_idf\n",
    "train.ix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF: product descriptions (not all -- only those with product titles) + product titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF: product titles alone (no descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF: product titles, descriptions, and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF sans stopwords (does it make any difference?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dictionary of word-count pairs\n",
    "words_product_descriptions = ''.join(product_descriptions['product_description']).split()\n",
    "words_train = ''.join(train['product_title']).split()\n",
    "d = Counter(words_product_descriptions + words_train) # Not lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for calculating TF-IDF score for doc, query pair\n",
    "\n",
    "def calc(doc, query):\n",
    "    doc = [w.lower() for w in doc.split()]\n",
    "    counts = Counter(doc)\n",
    "    query = query.lower().split()\n",
    "    return sum([counts[word] * log(N / (1+d[word])) for word in set(query) & set(doc)]) # 1 to avoid div by 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = OrderedDict()\n",
    "\n",
    "for row in zip(train['id'], train['product_uid'], train['product_title'], train['search_term']):\n",
    "    _id, pid, pt, st = row\n",
    "    TF = Counter(pt.split())\n",
    "    score = sum([calc(pt, word) for word in st.lower().split()])\n",
    "    scores[_id] = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TF-IDF sans stopwords (any difference?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE: JACCARD DISTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def jaccard_dist(phrase1, phrase2):\n",
    "#     '''Returns the Jaccard distance for two phrases, eg, search query and product title'''\n",
    "#     lst1, lst2 = set(phrase1.lower().split()), set(phrase2.lower().split())\n",
    "#     return float(len(lst1 & lst2)) / len(lst1 | lst2)\n",
    "\n",
    "# def words_shared(st, pt):\n",
    "#     st_list = re.split('\\s', st.lower())\n",
    "#     pt_list = re.split('\\s', pt.lower())\n",
    "#     dist = jaccard_dist(st_list, pt_list)\n",
    "#     return dist\n",
    "\n",
    "# train['jaccard'] = pd.Series([words_shared(st, pt) \n",
    "#                       for st, pt in zip(train['search_term'], train['product_title'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BAG OF WORDS: product descriptions, attributes, titles -- product by product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### DEFAULT PENALTY: products that appear too many times -- and have low relevance -- suggesting that they are defaults for when the search engine is stumped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aggregate on product ID's themselves and penalize too popular products.\n",
    "from collections import Counter\n",
    "d = Counter(train['product_uid'])\n",
    "for pid in d: \n",
    "    if d[pid] > 5:\n",
    "        pass\n",
    "#         print(pid, d[pid],) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare to divide the dataset into test, train (lifted from sklearn Iris example)\n",
    "train['is_train'] = np.random.uniform(0, 1, len(train)) <= .75\n",
    "\n",
    "# Split data into training and test\n",
    "train_data, test_data = train[train['is_train']==True], train[train['is_train']==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1(ct, preds):\n",
    "    '''Returns the F1 score given the confusion matrix generated with a set of predictions'''\n",
    "    index = list(pd.crosstab(test_data['relevance_rounded'], preds))\n",
    "    total = 0\n",
    "    for i in list(ct):\n",
    "        total += sum(list(ct[i]))\n",
    "\n",
    "    true_positives, false_positives, true_negatives, false_negatives = 0, 0, 0, 0\n",
    "\n",
    "    for i in zip(list(ct), range(1, len(ct)+1)): \n",
    "        row = list(ct.ix[i[1]])\n",
    "        column = list(ct[i[0]])\n",
    "    \n",
    "        tp = row[i[1]-1]\n",
    "        true_positives  += tp\n",
    "        false_negatives += sum(row)-tp\n",
    "        false_positives += sum(column) - tp\n",
    "        true_negatives = total - tp\n",
    "    \n",
    "    return float(2*true_positives) / (2 * true_positives + false_positives + false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'product_uid', 'product_title', 'search_term', 'relevance',\n",
       "       'relevance_rounded', 'relevance_rounded_quarters', 'exact',\n",
       "       'overlapping_words', 'percentage_overlapping_words', 'tf_idf',\n",
       "       'is_train'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_forest(features):\n",
    "    # clf = RandomForestClassifier(n_jobs=2)\n",
    "    clf = RandomForestClassifier(n_jobs=2, class_weight = 'balanced', oob_score = False, criterion = 'entropy')\n",
    "\n",
    "    y, _ = pd.factorize(train_data['relevance_rounded']) # 0, 1, 2\n",
    "\n",
    "    clf.fit(train_data[features], y)\n",
    "\n",
    "    # Predicting on those features will output predictions that match y\n",
    "    preds = clf.predict(test_data[features])\n",
    "\n",
    "    # target_names = test['relevance_rounded']\n",
    "    target_names = ['1', '2', '3']\n",
    "    out = [target_names[pred] for pred in preds]\n",
    "\n",
    "    # preds = target_names[clf.predict(test[features])]\n",
    "\n",
    "    preds = clf.predict(test_data[features])\n",
    "\n",
    "    pd.crosstab(test_data['relevance_rounded'], np.asarray(out), rownames=['actual'], colnames=['preds'])\n",
    "    \n",
    "    return (ct, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['exact', 'overlapping_words', 'percentage_overlapping_words', 'tf_idf'], dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Features\n",
    "train.columns[7:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds      1     2     3\n",
      "actual                  \n",
      "1        280   159   873\n",
      "2       2292  2167  4086\n",
      "3       1830  4030  2749\n",
      "preds      1     2     3\n",
      "actual                  \n",
      "1        280   159   873\n",
      "2       2292  2167  4086\n",
      "3       1830  4030  2749\n",
      "preds      1     2     3\n",
      "actual                  \n",
      "1        280   159   873\n",
      "2       2292  2167  4086\n",
      "3       1830  4030  2749\n",
      "preds      1     2     3\n",
      "actual                  \n",
      "1        280   159   873\n",
      "2       2292  2167  4086\n",
      "3       1830  4030  2749\n",
      "preds      1     2     3\n",
      "actual                  \n",
      "1        280   159   873\n",
      "2       2292  2167  4086\n",
      "3       1830  4030  2749\n",
      "preds      1     2     3\n",
      "actual                  \n",
      "1        280   159   873\n",
      "2       2292  2167  4086\n",
      "3       1830  4030  2749\n",
      "preds      1     2     3\n",
      "actual                  \n",
      "1        280   159   873\n",
      "2       2292  2167  4086\n",
      "3       1830  4030  2749\n",
      "preds      1     2     3\n",
      "actual                  \n",
      "1        280   159   873\n",
      "2       2292  2167  4086\n",
      "3       1830  4030  2749\n",
      "preds      1     2     3\n",
      "actual                  \n",
      "1        280   159   873\n",
      "2       2292  2167  4086\n",
      "3       1830  4030  2749\n",
      "preds      1     2     3\n",
      "actual                  \n",
      "1        280   159   873\n",
      "2       2292  2167  4086\n",
      "3       1830  4030  2749\n",
      "preds      1     2     3\n",
      "actual                  \n",
      "1        280   159   873\n",
      "2       2292  2167  4086\n",
      "3       1830  4030  2749\n",
      "preds      1     2     3\n",
      "actual                  \n",
      "1        280   159   873\n",
      "2       2292  2167  4086\n",
      "3       1830  4030  2749\n",
      "preds      1     2     3\n",
      "actual                  \n",
      "1        280   159   873\n",
      "2       2292  2167  4086\n",
      "3       1830  4030  2749\n",
      "preds      1     2     3\n",
      "actual                  \n",
      "1        280   159   873\n",
      "2       2292  2167  4086\n",
      "3       1830  4030  2749\n",
      "preds      1     2     3\n",
      "actual                  \n",
      "1        280   159   873\n",
      "2       2292  2167  4086\n",
      "3       1830  4030  2749\n"
     ]
    }
   ],
   "source": [
    "for n in range(1, 5):\n",
    "    for comb in combinations([7, 8, 9, 10], n):\n",
    "#         print(comb)\n",
    "        features = train.columns[list(comb)]\n",
    "        ct, preds = random_forest(features)\n",
    "#         print(comb, f1(ct, preds))\n",
    "        print(ct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# THINGS TO TRY / CONTEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What do 3's have in common?\n",
    "# What do 1's have in common?\n",
    "# QUERIES CONTAINING WORDS NOT IN DICTIONARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
