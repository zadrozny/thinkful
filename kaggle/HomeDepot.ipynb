{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOME DEPOT KAGGLE COMPETITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "from itertools import combinations\n",
    "from math import log\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product_descriptions = pd.read_csv('DATA/product_descriptions.csv', header=0, encoding = \"ISO-8859-1\")\n",
    "attributes = pd.read_csv('DATA/attributes.csv', header=0, encoding = \"ISO-8859-1\")\n",
    "train = pd.read_csv('DATA/train.csv', header=0,  encoding = \"ISO-8859-1\")\n",
    "test = pd.read_csv('DATA/test.csv', header=0,  encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NORMALIZE EVALUATOR SCORES\n",
    "* Evaluator scores are are continuous averages on the interval [1, 3]; the classifier seems unable to handles this\n",
    "* relevance_rounded consists of values: 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since relevance is an aggregate, values are continuous rather than discrete\n",
    "# It appears the classifier can only handle a limited number of values\n",
    "# So we round up\n",
    "train['relevance_rounded'] = train['relevance'].map(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note imbalance of 1's, 2's, and 3's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 5115, 2.0: 34614, 3.0: 34338})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train['relevance_rounded'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE FEATURES\n",
    "\n",
    "### FEATURE: EXACT MATCH (_boolean_)\n",
    "* between search term (as phrase) and product title, allowing only differences in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['exact'] = pd.Series([True if st.lower() in pt.lower() else False \n",
    "                      for st, pt in zip(train['search_term'], train['product_title'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: OVERLAPPING WORDS (_count >= 0_)\n",
    "* between search term (as phrase) and product title, allowing only differences in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['overlapping_words'] = pd.Series([len(set(st.lower().split()) & set(pt.lower().split()))\n",
    "                      for st, pt in zip(train['search_term'], train['product_title'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: OVERLAPPING WORDS (_percentage_)\n",
    "* allowing for differences in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['percentage_overlapping_words'] = pd.Series([len(set(st.lower().split()) & \n",
    "                                                       set(pt.lower().split()))/float(len(st.split()))\n",
    "                                                       for st, pt in zip(train['search_term'], train['product_title'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc(doc, query, N=1, D={}):\n",
    "    '''Calculates TF-IDF score for doc, query pair, given a document count (N) and dictionary of word counts (D)'''\n",
    "    doc = [w.lower() for w in doc.split()]\n",
    "    counts = Counter(doc)\n",
    "    query = query.lower().split()\n",
    "    return sum([counts[word] * log(N / (1+D[word])) for word in set(query) & set(doc)]) # 1 to avoid div by 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: TF-IDF: product descriptions (all) + product titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of documents\n",
    "N = len(set(product_descriptions['product_uid'] + train['product_uid']))\n",
    "\n",
    "# Dictionary of word-count pairs\n",
    "words_product_descriptions = ''.join(product_descriptions['product_description']).split()\n",
    "words_train = ''.join(train['product_title']).split()\n",
    "D = Counter(words_product_descriptions + words_train) # Not lowercased\n",
    "\n",
    "scores = OrderedDict()\n",
    "for row in zip(train['id'], train['product_uid'], train['product_title'], train['search_term']):\n",
    "    _id, pid, pt, st = row\n",
    "    TF = Counter(pt.split())\n",
    "    score = sum([calc(pt, word, N, D) for word in st.lower().split()])\n",
    "    scores[_id] = score\n",
    "\n",
    "# Vector\n",
    "train['tf_idf'] = list(scores.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: TF-IDF: product titles, descriptions, and attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dictionary of word-count pairs\n",
    "words_product_descriptions = ''.join(product_descriptions['product_description']).split()\n",
    "words_train = ''.join(train['product_title']).split()\n",
    "words_attributes = ''.join(str(at) for at in attributes['value']).split()\n",
    "d_titles_descriptions_attr = Counter(words_product_descriptions + words_train + words_attributes) # Not lowercased\n",
    "\n",
    "scores_titles_descriptions_attr = OrderedDict()\n",
    "\n",
    "for row in zip(train['id'], train['product_uid'], train['product_title'], train['search_term'], attributes['value']):\n",
    "    _id, pid, pt, st, attr = row\n",
    "    TF = Counter(pt.split())\n",
    "    score = sum([calc(pt, word, N, d_titles_descriptions_attr) for word in st.lower().split()])\n",
    "    scores_titles_descriptions_attr[_id] = score\n",
    "\n",
    "train['tf_titles_descriptions_attr'] = list(scores_titles_descriptions_attr.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE: JACCARD DISTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def jaccard_dist(phrase1, phrase2):\n",
    "    '''Returns the Jaccard distance for two phrases, eg, search query and product title'''\n",
    "    lst1, lst2 = set(phrase1), set(phrase2)\n",
    "    return float(len(lst1 & lst2)) / len(lst1 | lst2)\n",
    "\n",
    "def words_shared(st, pt):\n",
    "    st_list = re.split('\\s', st.lower())\n",
    "    pt_list = re.split('\\s', pt.lower())\n",
    "    dist = jaccard_dist(st_list, pt_list)\n",
    "    return dist\n",
    "\n",
    "train['jaccard'] = pd.Series([words_shared(st, pt) \n",
    "                      for st, pt in zip(train['search_term'], train['product_title'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CAPPED\n",
    "* Where the number of 1's, 2's, and 3's is equal\n",
    "* Capping at this stage so that all the feature vectors are captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ones = train[train.relevance_rounded == 1].sample(n=len(train[train.relevance_rounded == 1]))\n",
    "twos = train[train.relevance_rounded == 2].sample(n=len(train[train.relevance_rounded == 1]))\n",
    "threes = train[train.relevance_rounded == 3].sample(n=len(train[train.relevance_rounded == 1]))\n",
    "capped = pd.concat([ones, twos, threes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT THE DATA, DEVISE A METRIC, and CREATE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into train & test, whole (uncapped) set and _capped_ (subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncapped (_uneven_ number of 1's, 2's, and 3's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare to divide the dataset into test, train (lifted from sklearn Iris example)\n",
    "train['is_train'] = np.random.uniform(0, 1, len(train)) <= .75\n",
    "\n",
    "# Split data into training and test\n",
    "train_data, test_data = train[train['is_train']==True], train[train['is_train']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 3837, 2.0: 25929, 3.0: 25766})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(list(train_data['relevance_rounded']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capped (_even_ number of 1's, 2's and 3's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare to divide the dataset into test, train (lifted from sklearn Iris example)\n",
    "capped['is_train'] = np.random.uniform(0, 1, len(capped)) <= .75\n",
    "\n",
    "# Split data into training and test\n",
    "train_data_capped, test_data_capped = capped[capped['is_train']==True], capped[capped['is_train']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 3899, 2.0: 3816, 3.0: 3790})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(list(train_data_capped['relevance_rounded']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METRIC: F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1(ct, preds, data):\n",
    "    '''Returns the F1 score given the confusion matrix generated with a set of predictions'''\n",
    "    index = list(pd.crosstab(data['relevance_rounded'], preds))\n",
    "    total = 0\n",
    "    for i in list(ct):\n",
    "        total += sum(list(ct[i]))\n",
    "\n",
    "    true_positives, false_positives, true_negatives, false_negatives = 0, 0, 0, 0\n",
    "\n",
    "    for i in zip(list(ct), range(1, len(ct)+1)): \n",
    "        row = list(ct.ix[i[1]])\n",
    "        column = list(ct[i[0]])\n",
    "    \n",
    "        tp = row[i[1]-1]\n",
    "        true_positives  += tp\n",
    "        false_negatives += sum(row)-tp\n",
    "        false_positives += sum(column) - tp\n",
    "        true_negatives = total - tp\n",
    "    \n",
    "    return float(2*true_positives) / (2 * true_positives + false_positives + false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 1: RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_forest(features, training_data, testing_data, y_):\n",
    "    clf = RandomForestClassifier(n_jobs=2, class_weight = 'balanced', oob_score = False, criterion = 'entropy')\n",
    "\n",
    "    clf.fit(training_data[features], y_)\n",
    "\n",
    "    preds = clf.predict(testing_data[features])\n",
    "\n",
    "    target_names = ['1', '2', '3']\n",
    "    out = [target_names[pred] for pred in preds]\n",
    "\n",
    "    ct = pd.crosstab(testing_data['relevance_rounded'], np.asarray(out), rownames=['actual'], colnames=['preds'])\n",
    "    \n",
    "    return (ct, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 2: NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def naive_bayes(features, training_data, testing_data, y_):\n",
    "    clf = GaussianNB()\n",
    "\n",
    "    clf.fit(training_data[features], y_)\n",
    "\n",
    "    preds = clf.predict(testing_data[features])\n",
    "\n",
    "    target_names = ['1', '2', '3']\n",
    "    out = [target_names[pred] for pred in preds]\n",
    "\n",
    "    ct = pd.crosstab(testing_data['relevance_rounded'], np.asarray(out), rownames=['actual'], colnames=['preds'])\n",
    "    \n",
    "    return(ct, preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_uncapped, _ = pd.factorize(train_data['relevance_rounded']) # 0, 1, 2\n",
    "y_capped, _ = pd.factorize(train_data_capped['relevance_rounded']) # 0, 1, 2\n",
    "\n",
    "\n",
    "scores = []\n",
    "for n in range(1, 6):\n",
    "    for comb in combinations([6, 7, 8, 9, 10, 11], n):\n",
    "        features = train.columns[list(comb)]\n",
    "        \n",
    "        # Random Forest\n",
    "        rf_ct, rf_preds = random_forest(features, train_data, test_data, y_uncapped)\n",
    "        rf_ct_capped, rf_preds_capped = random_forest(features, train_data_capped, test_data_capped, y_capped)\n",
    "\n",
    "        rf_f1_uncapped = f1(rf_ct, rf_preds, test_data)\n",
    "        rf_f1_capped = f1(rf_ct_capped, rf_preds_capped, test_data_capped)\n",
    "        \n",
    "        # Naive Bayes\n",
    "        nb_ct, nb_preds = naive_bayes(features, train_data, test_data, y_uncapped)\n",
    "        nb_ct_capped, nb_preds_capped = naive_bayes(features, train_data_capped, test_data_capped, y_capped)\n",
    "\n",
    "        nb_f1_uncapped = f1(nb_ct, nb_preds, test_data)\n",
    "        nb_f1_capped = f1(nb_ct_capped, nb_preds_capped, test_data_capped)\n",
    "        \n",
    "\n",
    "        scores.append([list(features), \n",
    "                       rf_ct, \n",
    "                       rf_preds, \n",
    "                       rf_f1_uncapped, \n",
    "                       rf_ct_capped, \n",
    "                       rf_preds_capped, \n",
    "                       rf_f1_capped, \n",
    "                       \n",
    "                       nb_ct, \n",
    "                       nb_preds, \n",
    "                       nb_f1_uncapped, \n",
    "                       nb_ct_capped, \n",
    "                       nb_preds_capped, \n",
    "                       nb_f1_capped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores, columns = ['features', 'rf_ct', 'rf_preds', 'rf_f1', \n",
    "                                            'rf_ct_capped', 'rf_preds_capped', 'rf_f1_capped', \n",
    "                                            \n",
    "                                            'nb_ct', 'nb_preds', 'nb_f1', \n",
    "                                            'nb_ct_capped', 'nb_preds_capped', 'nb_f1_capped'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEAN SCORES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Mean F1 for Whole (Uncapped) Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28067292160986529"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_mean_f1 = sum(scores_df['rf_f1'])/len(scores_df)\n",
    "rf_mean_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Mean F1 (Uncapped) Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43886827696742492"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_mean_f1 = sum(scores_df['nb_f1'])/len(scores)\n",
    "nb_mean_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Mean F1 for Capped Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45141146877617322"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_mean_f1_capped = sum(scores_df['rf_f1_capped'])/len(scores_df)\n",
    "rf_mean_f1_capped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Mean F1 for Capped Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4299712874778982"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_mean_f1_capped = sum(scores_df['nb_f1_capped'])/len(scores_df)\n",
    "nb_mean_f1_capped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All Data</th>\n",
       "      <th>Capped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.280673</td>\n",
       "      <td>0.451411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.438868</td>\n",
       "      <td>0.429971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               All Data    Capped\n",
       "Random Forest  0.280673  0.451411\n",
       "Naive Bayes    0.438868  0.429971"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = pd.DataFrame([[rf_mean_f1, rf_mean_f1_capped], [nb_mean_f1, nb_mean_f1_capped]], \n",
    "                   index=['Random Forest', 'Naive Bayes'],\n",
    "                    columns = ['All Data', 'Capped'])\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEST SCORES: RF vs NB, on WHOLE & CAPPED DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Best Score & Corresponding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rf_f1</th>\n",
       "      <th>features</th>\n",
       "      <th>rf_ct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.334179</td>\n",
       "      <td>[exact, overlapping_words]</td>\n",
       "      <td>preds      1     2     3\n",
       "actual               ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rf_f1                    features  \\\n",
       "6  0.334179  [exact, overlapping_words]   \n",
       "\n",
       "                                               rf_ct  \n",
       "6  preds      1     2     3\n",
       "actual               ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df['rf_ct_width'] = pd.Series([len(np.array(row)[0]) for row in scores_df.rf_ct])\n",
    "tmp = scores_df[scores_df['rf_ct_width'] > 2] # Classifying into three features\n",
    "tmp[tmp['rf_f1'] == max(tmp['rf_f1'])][['rf_f1', 'features', 'rf_ct']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Best _Capped_ Score & Corresponding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rf_f1_capped</th>\n",
       "      <th>features</th>\n",
       "      <th>rf_ct_capped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.483073</td>\n",
       "      <td>[exact, overlapping_words, percentage_overlapp...</td>\n",
       "      <td>preds     1    2    3\n",
       "actual               \n",
       "1 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rf_f1_capped                                           features  \\\n",
       "56      0.483073  [exact, overlapping_words, percentage_overlapp...   \n",
       "\n",
       "                                         rf_ct_capped  \n",
       "56  preds     1    2    3\n",
       "actual               \n",
       "1 ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[scores_df['rf_f1_capped'] == max(scores_df['rf_f1_capped'])][['rf_f1_capped', 'features', 'rf_ct_capped']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Best Score & Corresponding Features\n",
    "* (Where the classifier is classifying into three classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_f1</th>\n",
       "      <th>features</th>\n",
       "      <th>nb_ct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.327057</td>\n",
       "      <td>[exact, overlapping_words, tf_idf, tf_titles_d...</td>\n",
       "      <td>preds      1     2     3\n",
       "actual               ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nb_f1                                           features  \\\n",
       "59  0.327057  [exact, overlapping_words, tf_idf, tf_titles_d...   \n",
       "\n",
       "                                                nb_ct  \n",
       "59  preds      1     2     3\n",
       "actual               ...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df['nb_ct_width'] = pd.Series([len(np.array(row)[0]) for row in scores_df.nb_ct])\n",
    "tmp = scores_df[scores_df['nb_ct_width'] > 2] # Classifying into three features\n",
    "tmp[tmp['nb_f1'] == max(tmp['nb_f1'])][['nb_f1', 'features', 'nb_ct']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Best _Capped_ Score & Corresponding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nb_f1_capped</th>\n",
       "      <th>features</th>\n",
       "      <th>nb_ct_capped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.449479</td>\n",
       "      <td>[percentage_overlapping_words, tf_idf]</td>\n",
       "      <td>preds     1    2    3\n",
       "actual               \n",
       "1 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    nb_f1_capped                                features  \\\n",
       "15      0.449479  [percentage_overlapping_words, tf_idf]   \n",
       "\n",
       "                                         nb_ct_capped  \n",
       "15  preds     1    2    3\n",
       "actual               \n",
       "1 ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[scores_df['nb_f1_capped'] == max(scores_df['nb_f1_capped'])][['nb_f1_capped', 'features', 'nb_ct_capped']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
